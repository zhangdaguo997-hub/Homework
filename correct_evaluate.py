# correct_evaluate.py
import json
import tempfile
import subprocess
import os
import re
from concurrent.futures import ThreadPoolExecutor, as_completed

def extract_function_name(completion):
    """ä»ç”Ÿæˆçš„ä»£ç ä¸­æå–å‡½æ•°å"""
    match = re.search(r'def\s+(\w+)', completion)
    return match.group(1) if match else None

def correct_check_humaneval_task(task_data, timeout=10):
    """
    æ­£ç¡®æ£€æŸ¥HumanEvalä»»åŠ¡ï¼šç¡®ä¿æµ‹è¯•ç”¨ä¾‹çœŸæ­£è¢«æ‰§è¡Œ
    """
    task_id = task_data["task_id"]
    prompt = task_data.get('prompt', '')
    completion = task_data.get('completion', '')
    test_cases = task_data.get('test', '')
    
    if not completion or completion.strip() == "error":
        return {
            "task_id": task_id,
            "passed": False,
            "reason": "empty_or_error"
        }
    
    # æå–å‡½æ•°å
    function_name = extract_function_name(completion)
    if not function_name:
        return {
            "task_id": task_id,
            "passed": False,
            "reason": "no_function_found"
        }
    
    # æ„å»ºå®Œæ•´ç¨‹åºï¼šprompt + completion + test_cases + è°ƒç”¨checkå‡½æ•°
    full_program = prompt + completion + "\n\n" + test_cases + f"\n\n# æ‰§è¡Œæµ‹è¯•\ncheck({function_name})"
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
        f.write(full_program)
        temp_file = f.name
    
    try:
        result = subprocess.run(
            ['python', temp_file],
            capture_output=True,
            text=True,
            timeout=timeout
        )
        
        # ä¸¥æ ¼çš„é€šè¿‡æ¡ä»¶ï¼šè¿”å›ç ä¸º0ä¸”æ²¡æœ‰æ–­è¨€é”™è¯¯
        passed = (
            result.returncode == 0 and 
            "AssertionError" not in result.stderr and
            "Traceback" not in result.stderr
        )
        
        return {
            "task_id": task_id,
            "passed": passed,
            "function_name": function_name,
            "returncode": result.returncode,
            "stdout": result.stdout[:200] if result.stdout else "",
            "stderr": result.stderr[:200] if result.stderr else "",
            "has_assertion_error": "AssertionError" in result.stderr,
            "has_traceback": "Traceback" in result.stderr
        }
        
    except subprocess.TimeoutExpired:
        return {
            "task_id": task_id,
            "passed": False,
            "reason": "timeout"
        }
    except Exception as e:
        return {
            "task_id": task_id,
            "passed": False,
            "reason": f"exception: {str(e)}"
        }
    finally:
        try:
            os.unlink(temp_file)
        except:
            pass

def correct_evaluate():
    """æ­£ç¡®è¯„ä¼°æ‰€æœ‰ä»»åŠ¡"""
    
    with open('humaneval_output.jsonl', 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    samples = [json.loads(line) for line in lines]
    print(f"ğŸ“Š æ­£ç¡®è¯„ä¼° {len(samples)} ä¸ªä»»åŠ¡")
    print("ğŸ”„ è¿™æ¬¡ä¼šçœŸæ­£æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹...")
    
    results = []
    passed_count = 0
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=4) as executor:
        future_to_task = {executor.submit(correct_check_humaneval_task, sample): sample for sample in samples}
        
        completed = 0
        for future in as_completed(future_to_task):
            result = future.result()
            results.append(result)
            
            if result["passed"]:
                passed_count += 1
                print(f"âœ… {completed+1:3d}/{len(samples)}: {result['task_id']} - é€šè¿‡")
            else:
                reason = result.get('reason', '')
                if result.get('has_assertion_error'):
                    reason = "æ–­è¨€å¤±è´¥"
                elif result.get('has_traceback'):
                    reason = "è¿è¡Œæ—¶é”™è¯¯"
                print(f"âŒ {completed+1:3d}/{len(samples)}: {result['task_id']} - å¤±è´¥ ({reason})")
            
            completed += 1
    
    pass_rate = passed_count / len(samples) * 100
    
    print(f"\nğŸ¯ æ­£ç¡®è¯„ä¼°ç»“æœ:")
    print(f"  æ€»ä»»åŠ¡æ•°: {len(samples)}")
    print(f"  é€šè¿‡æ•°: {passed_count}")
    print(f"  Pass@1: {pass_rate:.2f}%")
    
    # è¯¦ç»†ç»Ÿè®¡
    reasons = {}
    for result in results:
        reason = result.get('reason', 'test_failed')
        if result['passed']:
            reason = 'passed'
        elif result.get('has_assertion_error'):
            reason = 'assertion_error'
        elif result.get('has_traceback'):
            reason = 'runtime_error'
        reasons[reason] = reasons.get(reason, 0) + 1
    
    print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡:")
    for reason, count in reasons.items():
        print(f"  {reason}: {count}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open('correct_evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
    summary = {
        "total_tasks": len(samples),
        "passed_tasks": passed_count,
        "pass_rate": pass_rate,
        "failure_breakdown": reasons,
        "model": "deepseek-chat",
        "evaluation_method": "correct_with_test_execution"
    }
    
    with open('correct_evaluation_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: correct_evaluation_results.json")
    print(f"ğŸ“‹ æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜åˆ°: correct_evaluation_summary.json")
    
    return pass_rate

if __name__ == "__main__":
    correct_evaluate()